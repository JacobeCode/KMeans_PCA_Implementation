{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobeCode/ML-AI/blob/main/trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE6Pbh0GOoMv",
        "outputId": "fab97bb8-8d28-4579-a53d-8a5e7eca2b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "cNIsbhEpt0Kb",
        "outputId": "ae654a0e-24e3-4301-9f2b-56489a0b34c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Run with UUID 03c026f8025545ffb041910e333c8bbb is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c143746bee6c>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mtrain_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Manually setting train_test sets (not with sklearn train_test_split because train/test sets are prepared in dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, tags, description, log_system_metrics)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mexperiment_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_active_run_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m    306\u001b[0m             (\n\u001b[1;32m    307\u001b[0m                 \u001b[0;34m\"Run with UUID {} is already active. To start a new run, first end the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Run with UUID 03c026f8025545ffb041910e333c8bbb is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import mlflow\n",
        "import scipy.stats\n",
        "import pandas\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from mlflow.models import infer_signature\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, make_scorer, confusion_matrix, log_loss, precision_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "database_dir='/MyDrive/Sentiment_Tests/database'\n",
        "\n",
        "# This is set to save files, data and artifacts to Google Drive without tuneling (download files and visualize/analize localy)\n",
        "# IMPORTANT - if you want to see artifacts and other data there is need to change \"meta.yml\" directories to your local equivalents\n",
        "# Maybe later I'll do automatic version\n",
        "mlflow.set_tracking_uri(\"/content/drive/MyDrive/mlruns\")\n",
        "\n",
        "mlflow.set_experiment(\"RandomForestClassificator\")\n",
        "\n",
        "# Labeling and sorting through data\n",
        "f=open('/content/drive/MyDrive/Sentiment_Tests/dataset/train.txt', \"r\")\n",
        "train_data_unclean=f.read()\n",
        "train_data=train_data_unclean.split(\"\\n\")\n",
        "for num, item in enumerate(train_data):\n",
        "    train_data[num]=item.split(\";\")\n",
        "\n",
        "data=[]\n",
        "labels=[]\n",
        "for item in train_data:\n",
        "    data.append(item[0])\n",
        "    labels.append(item[1])\n",
        "\n",
        "# Labeling and sorting through data (test_data)\n",
        "f=open('/content/drive/MyDrive/Sentiment_Tests/dataset/test.txt', \"r\")\n",
        "test_data_unclean=f.read()\n",
        "test_data=test_data_unclean.split(\"\\n\")\n",
        "for num, item in enumerate(test_data):\n",
        "    test_data[num]=item.split(\";\")\n",
        "\n",
        "test_dataset=[]\n",
        "test_labels=[]\n",
        "for item in test_data:\n",
        "    test_dataset.append(item[0])\n",
        "    test_labels.append(item[1])\n",
        "\n",
        "\n",
        "# Converting simple arrays to DataFrame\n",
        "train_database=pandas.DataFrame(data=data)\n",
        "train_database[\"sentiment\"]=labels\n",
        "train_database.columns=[\"text_data\", \"sentiment\"]\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Manually setting train_test sets (not with sklearn train_test_split because train/test sets are prepared in dataset)\n",
        "    X_train=data\n",
        "    y_train=labels\n",
        "    X_test=test_dataset\n",
        "    y_test=test_labels\n",
        "\n",
        "    params={\n",
        "        \"criterion\": \"gini\",\n",
        "        \"n_estimators\": 150,\n",
        "    }\n",
        "\n",
        "    # Model creation\n",
        "    model=RandomForestClassifier(**params)\n",
        "\n",
        "    # Vectorizing of text data\n",
        "    count_vec=CountVectorizer()\n",
        "    X_train_counts=count_vec.fit_transform(X_train)\n",
        "    X_test_counts=count_vec.transform(X_test)\n",
        "\n",
        "    # Train fit and prediction of model\n",
        "    model.fit(X_train_counts, y_train)\n",
        "    preds_test=model.predict(X_test_counts)\n",
        "\n",
        "    # Accuracy score\n",
        "    accuracy=accuracy_score(y_test, preds_test)\n",
        "    print(accuracy)\n",
        "\n",
        "    mlflow.log_params(params)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.set_tag(\"Training Info\", \"Is it working through Cloud\")\n",
        "    signature=infer_signature(X_train, preds_test)\n",
        "    model_info=mlflow.sklearn.log_model(\n",
        "        sk_model=RandomForestClassifier,\n",
        "        artifact_path=\"sentiment_model\",\n",
        "        signature=signature,\n",
        "        input_example=X_train_counts,\n",
        "        registered_model_name=\"gini\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tLJkqXcOoMx"
      },
      "outputs": [],
      "source": [
        "# This is addition of tuneling through ngrok to look into training data with use of mlflow without Drive\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "get_ipython().system_raw('mlflow ui --port 5000 &')\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_AUTH_TOKEN'))\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Killing existing ngrok processes\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "9pq1Ireuoy1s"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}